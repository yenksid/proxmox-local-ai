#!/bin/bash

# ==============================================================================
# AUTOMATED INSTALLER: Local AI Server on Proxmox LXC
# Installs llama.cpp engine + Llama 3.2 3B Model + Systemd Service
# ==============================================================================

# Exit on any error
set -e

# Colors for output
GREEN='\033[0;32m'
BLUE='\033[0;34m'
RED='\033[0;31m'
NC='\033[0m' # No Color

echo -e "${BLUE}>>> Starting Installation of Local AI Server...${NC}"

# 1. Check Root
if [ "$EUID" -ne 0 ]; then
  echo -e "${RED}Please run as root (or use sudo).${NC}"
  exit 1
fi

# 2. System Update & Dependencies
echo -e "${GREEN}>>> Updating system and installing dependencies...${NC}"
apt-get update -qq
apt-get install -y -qq git build-essential cmake curl wget

# 3. Clone & Build Engine (llama.cpp)
INSTALL_DIR="/root/llama.cpp"
if [ -d "$INSTALL_DIR" ]; then
    echo -e "${BLUE}>>> llama.cpp directory exists. Pulling latest changes...${NC}"
    cd "$INSTALL_DIR"
    git pull
else
    echo -e "${GREEN}>>> Cloning llama.cpp repository...${NC}"
    git clone https://github.com/ggerganov/llama.cpp "$INSTALL_DIR"
    cd "$INSTALL_DIR"
fi

echo -e "${GREEN}>>> Compiling llama-server (This may take a few minutes)...${NC}"
rm -rf build
cmake -B build
# Use all available cores for compilation
CORES=$(nproc)
cmake --build build --config Release -j"$CORES" --target llama-server

# 4. Download Model (Llama 3.2 3B Q4)
MODEL_DIR="/root/models"
MODEL_URL="https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_M.gguf"
MODEL_PATH="$MODEL_DIR/llama-3.2-3b-q4.gguf"

mkdir -p "$MODEL_DIR"

if [ -f "$MODEL_PATH" ]; then
    echo -e "${BLUE}>>> Model already exists. Skipping download.${NC}"
else
    echo -e "${GREEN}>>> Downloading Llama 3.2 3B model...${NC}"
    wget "$MODEL_URL" -O "$MODEL_PATH"
fi

# 5. Create Startup Script
SCRIPT_PATH="/root/start_ai.sh"
echo -e "${GREEN}>>> Creating startup script at $SCRIPT_PATH...${NC}"

cat <<EOF > "$SCRIPT_PATH"
#!/bin/bash
# Auto-generated by install.sh
$INSTALL_DIR/build/bin/llama-server \\
    -m $MODEL_PATH \\
    --host 0.0.0.0 \\
    --port 8080 \\
    -c 4096 \\
    --temp 0.7 \\
    -t 4
EOF

chmod +x "$SCRIPT_PATH"

# 6. Create Systemd Service
SERVICE_PATH="/etc/systemd/system/local-ai.service"
echo -e "${GREEN}>>> Creating systemd service at $SERVICE_PATH...${NC}"

cat <<EOF > "$SERVICE_PATH"
[Unit]
Description=Local AI Server (Llama 3.2)
After=network.target

[Service]
Type=simple
User=root
WorkingDirectory=/root
ExecStart=$SCRIPT_PATH
Restart=always
RestartSec=3

[Install]
WantedBy=multi-user.target
EOF

# 7. Enable & Start Service
echo -e "${GREEN}>>> Enabling and starting service...${NC}"
systemctl daemon-reload
systemctl enable local-ai.service
systemctl restart local-ai.service

# 8. Final Info
IP_ADDR=$(hostname -I | awk '{print $1}')
echo -e "${GREEN}======================================================${NC}"
echo -e "${GREEN} INSTALLATION COMPLETE! ${NC}"
echo -e "${GREEN}======================================================${NC}"
echo -e "Your AI Server is running at: ${BLUE}http://$IP_ADDR:8080${NC}"
echo -e "API Endpoint: ${BLUE}http://$IP_ADDR:8080/v1${NC}"
echo -e "To check logs: ${BLUE}journalctl -u local-ai -f${NC}"