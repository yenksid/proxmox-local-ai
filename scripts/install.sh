#!/bin/bash

# ==============================================================================
# AUTOMATED INSTALLER: BitNet AI Server on Proxmox LXC
# Installs llama.cpp engine + Llama 3.2 3B Model + Web UI + Systemd Service
# Version: 1.5 (Repo-based UI Fetch + Safety Checks)
# ==============================================================================

# Exit on any error
set -e

# Colors for output
GREEN='\033[0;32m'
BLUE='\033[0;34m'
RED='\033[0;31m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

echo -e "${BLUE}>>> Starting Installation of BitNet AI Server...${NC}"

# 1. Check Root
if [ "$EUID" -ne 0 ]; then
  echo -e "${RED}Please run as root (or use sudo).${NC}"
  exit 1
fi

# 1.1 Safety Check: Warn if running on Proxmox Host
if [ -f "/etc/pve/storage.cfg" ] || [ -f "/etc/pve/local/pve-ssl.key" ]; then
    echo -e "${RED}==============================================================${NC}"
    echo -e "${RED}⚠️  WARNING: PROXMOX HOST DETECTED ⚠️${NC}"
    echo -e "${RED}==============================================================${NC}"
    echo -e "${YELLOW}It looks like you are running this script directly on your Proxmox Node.${NC}"
    echo -e "${YELLOW}This is NOT recommended. You should run this inside an LXC Container.${NC}"
    echo -e ""
    echo -e "Press Ctrl+C to abort, or ENTER to proceed (at your own risk)..."
    # Force read from /dev/tty to handle 'curl | bash' pipe correctly
    read -p "" < /dev/tty
fi

# 2. System Update & Dependencies
echo -e "${GREEN}>>> Updating system and installing dependencies...${NC}"
apt-get update -qq
apt-get install -y -qq git build-essential cmake curl wget libssl-dev pkg-config

# 3. Clone & Build Engine (llama.cpp)
INSTALL_DIR="/root/llama.cpp"
if [ -d "$INSTALL_DIR" ]; then
    echo -e "${BLUE}>>> llama.cpp directory exists. Pulling latest changes...${NC}"
    cd "$INSTALL_DIR"
    git pull
else
    echo -e "${GREEN}>>> Cloning llama.cpp repository...${NC}"
    # Retry logic + DNS fix attempt
    git clone https://github.com/ggerganov/llama.cpp "$INSTALL_DIR" || {
        echo -e "${RED}Git clone failed. Checking DNS...${NC}"
        echo "nameserver 8.8.8.8" > /etc/resolv.conf
        git clone https://github.com/ggerganov/llama.cpp "$INSTALL_DIR"
    }
    cd "$INSTALL_DIR"
fi

echo -e "${GREEN}>>> Compiling llama-server (This may take a few minutes)...${NC}"
rm -rf build
cmake -B build
CORES=$(nproc)
cmake --build build --config Release -j"$CORES" --target llama-server

# 4. Download Model (Llama 3.2 3B Q4)
MODEL_DIR="/root/models"
MODEL_URL="https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_M.gguf"
MODEL_PATH="$MODEL_DIR/llama-3.2-3b-q4.gguf"

mkdir -p "$MODEL_DIR"

if [ -f "$MODEL_PATH" ]; then
    echo -e "${BLUE}>>> Model already exists. Skipping download.${NC}"
else
    echo -e "${GREEN}>>> Downloading Llama 3.2 3B model...${NC}"
    wget "$MODEL_URL" -O "$MODEL_PATH"
fi

# 5. Fetch Web UI from Repository
PUBLIC_DIR="/root/public"
UI_URL="https://raw.githubusercontent.com/yenksid/proxmox-local-ai/main/public/index.html"

mkdir -p "$PUBLIC_DIR"
echo -e "${GREEN}>>> Fetching Web UI from repository...${NC}"

# Download index.html directly from GitHub
wget -q "$UI_URL" -O "$PUBLIC_DIR/index.html" || {
    echo -e "${RED}Failed to download UI from GitHub. Check network or repo URL.${NC}"
    echo -e "${YELLOW}Creating a basic fallback file...${NC}"
    echo "<h1>Error: UI Download Failed</h1><p>Check logs.</p>" > "$PUBLIC_DIR/index.html"
}

# 6. Create Startup Script
SCRIPT_PATH="/root/start_ai.sh"
echo -e "${GREEN}>>> Creating startup script at $SCRIPT_PATH...${NC}"

cat <<EOF > "$SCRIPT_PATH"
#!/bin/bash
# Auto-generated by install.sh
$INSTALL_DIR/build/bin/llama-server \\
    -m $MODEL_PATH \\
    --host 0.0.0.0 \\
    --port 8080 \\
    -c 4096 \\
    --temp 0.7 \
    -t 4 \\
    --path $PUBLIC_DIR
EOF

chmod +x "$SCRIPT_PATH"

# 7. Create Systemd Service
SERVICE_PATH="/etc/systemd/system/bitnet.service"
echo -e "${GREEN}>>> Creating systemd service at $SERVICE_PATH...${NC}"

cat <<EOF > "$SERVICE_PATH"
[Unit]
Description=BitNet AI Server (Llama 3.2)
After=network.target

[Service]
Type=simple
User=root
WorkingDirectory=/root
ExecStart=$SCRIPT_PATH
Restart=always
RestartSec=3

[Install]
WantedBy=multi-user.target
EOF

# 8. Enable & Start Service
echo -e "${GREEN}>>> Enabling and starting service...${NC}"
systemctl daemon-reload
systemctl enable bitnet.service
systemctl restart bitnet.service

# 9. Final Info
IP_ADDR=$(ip route get 1.1.1.1 2>/dev/null | grep -oP 'src \K\S+') || IP_ADDR=$(hostname -I | awk '{print $1}')

echo -e "${GREEN}======================================================${NC}"
echo -e "${GREEN} INSTALLATION COMPLETE! ${NC}"
echo -e "${GREEN}======================================================${NC}"
echo -e "Your AI Server is running at: ${BLUE}http://$IP_ADDR:8080${NC}"
echo -e "API Endpoint: ${BLUE}http://$IP_ADDR:8080/v1${NC}"
